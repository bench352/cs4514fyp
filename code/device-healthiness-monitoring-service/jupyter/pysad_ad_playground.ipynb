{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-time Anomaly Detection Playground\n",
    "\n",
    "Dataset used: [AnoML-IoT](https://www.kaggle.com/datasets/hkayan/anomliot)\n",
    "\n",
    "Sample dataset format:\n",
    "\n",
    "| Time       | Temperature | Humidity | ... |\n",
    "|------------|-------------|----------|-----|\n",
    "| 1623781306 | 37.94       | 28.94    | ... |\n",
    "| ...        | ...         | ...      | ... |\n",
    "\n",
    "Although the data is unlabelled, it was stated in the description of the dataset that anomalies are created in the following period of time:\n",
    "\n",
    "- 18:21:46 - 19:37:16 (first day)\n",
    "- 02:26:36 - 04:15:56 (second day)\n",
    "- 08:54:46 - 10:45:36 (second day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "anoml_iot_dataset = pd.read_csv(\"./datasets/dataset_final.csv\")\n",
    "anoml_iot_dataset\n",
    "\n",
    "# Selecting the date range to be used for training and testing\n",
    "anoml_iot_dataset = anoml_iot_dataset[anoml_iot_dataset[\"Time\"] > 1623787200]\n",
    "record_count = anoml_iot_dataset.shape[0]\n",
    "print(f\"Total number of records: {record_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_data_point_anomaly(row):\n",
    "    # 18:21:46 - 19:37:16 (first day)\n",
    "    if row[\"Time\"] >= 1623781306 and row[\"Time\"] <= 1623785836:\n",
    "        return 1\n",
    "    # 02:26:36 - 04:15:56 (second day)\n",
    "    if row[\"Time\"] >= 1623810396 and row[\"Time\"] <= 1623816956:\n",
    "        return 1\n",
    "    # 08:54:46 - 10:45:36 (second day)\n",
    "    if row[\"Time\"] >= 1623833686 and row[\"Time\"] <= 1623840336:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "ground_truth = []\n",
    "for _, row in anoml_iot_dataset.iterrows():\n",
    "    ground_truth.append(is_data_point_anomaly(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "timestamps = pd.to_datetime(anoml_iot_dataset[\"Time\"], unit=\"s\", origin=\"unix\")\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    6,\n",
    "    1,\n",
    "    figsize=(20, 30),\n",
    "    sharex=True,\n",
    "    gridspec_kw={\"height_ratios\": [5, 5, 5, 5, 5, 1]},\n",
    ")\n",
    "ax[0].set_title(\"Temperature\")\n",
    "ax[0].plot(timestamps, anoml_iot_dataset[\"Temperature\"], label=\"Temperature\")\n",
    "ax[1].set_title(\"Humidity\")\n",
    "ax[1].plot(timestamps, anoml_iot_dataset[\"Humidity\"], label=\"Humidity\")\n",
    "ax[2].set_title(\"Air Quality\")\n",
    "ax[2].plot(timestamps, anoml_iot_dataset[\"Air Quality\"], label=\"Air Quality\")\n",
    "ax[3].set_title(\"Light\")\n",
    "ax[3].plot(timestamps, anoml_iot_dataset[\"Light\"], label=\"Light\")\n",
    "ax[4].set_title(\"Loudness\")\n",
    "ax[4].plot(timestamps, anoml_iot_dataset[\"Loudness\"], label=\"Loudness\")\n",
    "ax[5].set_title(\"Anomaly Ground Truth\")\n",
    "ax[5].plot(timestamps, ground_truth, label=\"Anomaly Ground Truth\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to convert numeric threshold to binary output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def get_decision_by_threshold(x, upper=math.inf, lower=-math.inf):\n",
    "    return 1 if x > upper or x < lower else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Different Models\n",
    "\n",
    "> **Assumption:** No data present in the system initially for training the model. The model is trained as the data arrived (online learning approach).\n",
    "\n",
    "Evaluate method:\n",
    "- \"Temperature\", \"Humidity\" and \"Light\" are used from the dataset.\n",
    "- They are all treated as individual data stream (univariate data).\n",
    "- Initiate `defaultdict` for storing model for each data stream.\n",
    "- Simulating real-time AD by fitting and scoring data point one by one, then apply a threshold to the anomaly score to compute the binary output.\n",
    "- Compare the AD result from model to the ground truth, compute the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_column_to_be_used = [\"Temperature\", \"Humidity\", \"Light\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact-STORM\n",
    "\n",
    "`pysad.models.ExactStorm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "exact_storm_ts_values = defaultdict(list)\n",
    "exact_storm_scores = defaultdict(list)\n",
    "exact_storm_results = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pysad.models import ExactStorm\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "UPPER_THRESHOLD = 0.5\n",
    "\n",
    "exact_storm_model_map = defaultdict(lambda: ExactStorm(window_size=700, max_radius=0.6))\n",
    "\n",
    "\n",
    "def learning_and_detecting_anomaly(ts_column):\n",
    "    values = []\n",
    "    scores = []\n",
    "    results = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for _, row in anoml_iot_dataset.iterrows():\n",
    "        ts_value = row[ts_column]\n",
    "        features = np.array([ts_value])\n",
    "        score = exact_storm_model_map[ts_column].fit_score_partial(features)\n",
    "        scores.append(score)\n",
    "        result = get_decision_by_threshold(score, upper=UPPER_THRESHOLD)\n",
    "        values.append(ts_value)\n",
    "        results.append(result)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\n",
    "        f\"\"\"[Performance of {ts_column}]\n",
    "Time taken for all record (s): {{:.5f}}\n",
    "Avg time taken for each (s)  : {{:.10f}}\n",
    "\"\"\".format(\n",
    "            end_time - start_time, (end_time - start_time) / record_count\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return values, scores, results\n",
    "\n",
    "\n",
    "pool = multiprocessing.Pool(processes=3)\n",
    "results = pool.map(learning_and_detecting_anomaly, ts_column_to_be_used)\n",
    "\n",
    "for i, column in enumerate(ts_column_to_be_used):\n",
    "    (\n",
    "        exact_storm_ts_values[column],\n",
    "        exact_storm_scores[column],\n",
    "        exact_storm_results[column],\n",
    "    ) = results[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    len(ts_column_to_be_used), 1, figsize=(20, 10), sharex=True, gridspec_kw={}\n",
    ")\n",
    "for idx, column in enumerate(ts_column_to_be_used):\n",
    "    ax[idx].plot(timestamps, exact_storm_scores[column], label=column)\n",
    "    ax[idx].axhline(y=UPPER_THRESHOLD, color=\"r\", linestyle=\"-\", label=\"Threshold\")\n",
    "    ax[idx].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Chart for Comparing Ground Truth and Model Evaluation Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_name = \"Exact-STORM\"\n",
    "for column in ts_column_to_be_used:\n",
    "    fig, ax = plt.subplots(\n",
    "        3,\n",
    "        1,\n",
    "        figsize=(20, 10),\n",
    "        sharex=True,\n",
    "        gridspec_kw={\"height_ratios\": [9, 1, 1]},\n",
    "    )\n",
    "    ax[0].plot(timestamps, exact_storm_ts_values[column], label=column)\n",
    "    ax[0].legend()\n",
    "    ax[0].set_title(f\"Telemetry value of {column}\")\n",
    "    ax[1].plot(timestamps, ground_truth, label=\"Ground Truth\")\n",
    "    ax[1].set_title(\"Ground Truth of Anomaly\")\n",
    "    ax[2].plot(timestamps, exact_storm_results[column], label=\"Detected Anomaly\")\n",
    "    ax[2].set_title(f\"Detection Result from {model_name}\")\n",
    "\n",
    "    fig.suptitle(f\"{model_name} on {column}\", fontweight=\"bold\", fontsize=14)\n",
    "    fig.savefig(f\"./results/{model_name}_{column}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for column in ts_column_to_be_used:\n",
    "    print(\n",
    "        f\"Accuracy of {model_name} on {column}: {accuracy_score(ground_truth, exact_storm_results[column])}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forest Algorithm for Streaming Data using Sliding Window\n",
    "\n",
    "`pysad.models.IForestASD`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "i_forest_ts_values = defaultdict(list)\n",
    "i_forest_scores = defaultdict(list)\n",
    "i_forest_results = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pysad.models import IForestASD\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "LOWER_THRESHOLD = -0.1\n",
    "\n",
    "i_forest_model_map = defaultdict(lambda: IForestASD(window_size=2000))\n",
    "\n",
    "\n",
    "def learning_and_detecting_anomaly(ts_column):\n",
    "    values = []\n",
    "    scores = []\n",
    "    results = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for _, row in anoml_iot_dataset.iterrows():\n",
    "        ts_value = row[ts_column]\n",
    "        features = np.array([row[ts_column]])\n",
    "        score = i_forest_model_map[ts_column].fit_score_partial(features)\n",
    "        scores.append(score)  # Save the score\n",
    "        result = get_decision_by_threshold(score, lower=LOWER_THRESHOLD)\n",
    "        values.append(ts_value)\n",
    "        results.append(result)  # Save the evaluation result\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\n",
    "        f\"\"\"[Performance of {ts_column}]\n",
    "Time taken for all record (s): {{:.5f}}\n",
    "Avg time taken for each (s)  : {{:.10f}}\n",
    "\"\"\".format(\n",
    "            end_time - start_time, (end_time - start_time) / record_count\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return values, scores, results\n",
    "\n",
    "\n",
    "pool = multiprocessing.Pool(processes=3)\n",
    "output = pool.map(learning_and_detecting_anomaly, ts_column_to_be_used)\n",
    "\n",
    "for idx, column in enumerate(ts_column_to_be_used):\n",
    "    i_forest_ts_values[column] = output[idx][0]\n",
    "    i_forest_scores[column] = output[idx][1]\n",
    "    i_forest_results[column] = output[idx][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    len(ts_column_to_be_used), 1, figsize=(20, 10), sharex=True, gridspec_kw={}\n",
    ")\n",
    "\n",
    "for idx,column in enumerate(ts_column_to_be_used):\n",
    "    ax[idx].plot(timestamps, i_forest_scores[column], label=column)\n",
    "    ax[idx].axhline(y=LOWER_THRESHOLD, color=\"g\", linestyle=\"-\", label=\"Lower Threshold\")\n",
    "    ax[idx].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Chart for Comparing Ground Truth and Model Evaluation Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_name = \"IForest-ASD\"\n",
    "for column in ts_column_to_be_used:\n",
    "    fig, ax = plt.subplots(\n",
    "        4,\n",
    "        1,\n",
    "        figsize=(20, 10),\n",
    "        sharex=True,\n",
    "        gridspec_kw={\"height_ratios\": [7, 2, 1, 1]},\n",
    "    )\n",
    "    ax[0].plot(timestamps, i_forest_ts_values[column], label=column)\n",
    "    ax[0].legend()\n",
    "    ax[0].set_title(f\"Telemetry value of {column}\")\n",
    "    ax[1].plot(timestamps, i_forest_scores[column], label=\"Score\")\n",
    "    ax[1].axhline(y=LOWER_THRESHOLD, color=\"g\", linestyle=\"-\", label=\"Lower Threshold\")\n",
    "    ax[1].legend()\n",
    "    ax[1].set_title(f\"Score from {model_name}\")\n",
    "    ax[2].plot(timestamps, ground_truth, label=\"Ground Truth\")\n",
    "    ax[2].set_title(\"Ground Truth of Anomaly\")\n",
    "    ax[3].plot(timestamps, i_forest_results[column], label=\"Detected Anomaly\")\n",
    "    ax[3].set_title(f\"Detection Result from {model_name}\")\n",
    "\n",
    "    fig.suptitle(f\"{model_name} on {column}\", fontweight=\"bold\", fontsize=14)\n",
    "    fig.savefig(f\"./results/{model_name}_{column}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for column in ts_column_to_be_used:\n",
    "    print(\n",
    "        f\"Accuracy of {model_name} on {column}: {accuracy_score(ground_truth, i_forest_results[column])}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNNCAD\n",
    "\n",
    "`pysad.models.KNNCAD`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "knncad_ts_values = defaultdict(list)\n",
    "knncad_scores = defaultdict(list)\n",
    "knncad_results = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pysad.models import KNNCAD\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "UPPER_THRESHOLD = 0.8\n",
    "\n",
    "knncad_model_map = defaultdict(lambda: KNNCAD(probationary_period=500))\n",
    "\n",
    "\n",
    "def learning_and_detecting_anomaly(ts_column):\n",
    "    values = []\n",
    "    scores = []\n",
    "    results = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for _, row in anoml_iot_dataset.iterrows():\n",
    "        ts_value = row[ts_column]\n",
    "        features = np.array([row[ts_column]])\n",
    "        score = knncad_model_map[ts_column].fit_score_partial(features)\n",
    "        scores.append(score)  # Save the score\n",
    "        result = get_decision_by_threshold(score, upper=UPPER_THRESHOLD)\n",
    "        values.append(ts_value)\n",
    "        results.append(result)  # Save the evaluation result\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\n",
    "        f\"\"\"[Performance of {ts_column}]\n",
    "Time taken for all record (s): {{:.5f}}\n",
    "Avg time taken for each (s)  : {{:.10f}}\n",
    "\"\"\".format(\n",
    "            end_time - start_time, (end_time - start_time) / record_count\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return values, scores, results\n",
    "\n",
    "\n",
    "pool = multiprocessing.Pool(processes=3)\n",
    "output = pool.map(learning_and_detecting_anomaly, ts_column_to_be_used)\n",
    "\n",
    "for idx, column in enumerate(ts_column_to_be_used):\n",
    "    knncad_ts_values[column] = output[idx][0]\n",
    "    knncad_scores[column] = output[idx][1]\n",
    "    knncad_results[column] = output[idx][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    len(ts_column_to_be_used), 1, figsize=(20, 10), sharex=True, gridspec_kw={}\n",
    ")\n",
    "\n",
    "for idx, column in enumerate(ts_column_to_be_used):\n",
    "    ax[idx].plot(timestamps, knncad_scores[column], label=column)\n",
    "    ax[idx].axhline(\n",
    "        y=UPPER_THRESHOLD, color=\"r\", linestyle=\"-\", label=\"Upper Threshold\"\n",
    "    )\n",
    "    ax[idx].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Chart for Comparing Ground Truth and Model Evaluation Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_name = \"KNN-CAD\"\n",
    "for column in ts_column_to_be_used:\n",
    "    fig, ax = plt.subplots(\n",
    "        4,\n",
    "        1,\n",
    "        figsize=(20, 10),\n",
    "        sharex=True,\n",
    "        gridspec_kw={\"height_ratios\": [7, 2, 1, 1]},\n",
    "    )\n",
    "    ax[0].plot(timestamps, knncad_ts_values[column], label=column)\n",
    "    ax[0].legend()\n",
    "    ax[0].set_title(f\"Telemetry value of {column}\")\n",
    "    ax[1].plot(timestamps, knncad_scores[column], label=\"Score\")\n",
    "    ax[1].axhline(y=UPPER_THRESHOLD, color=\"r\", linestyle=\"-\", label=\"Upper Threshold\")\n",
    "    ax[1].legend()\n",
    "    ax[1].set_title(f\"Score from {model_name}\")\n",
    "    ax[2].plot(timestamps, ground_truth, label=\"Ground Truth\")\n",
    "    ax[2].set_title(\"Ground Truth of Anomaly\")\n",
    "    ax[3].plot(timestamps, knncad_results[column], label=\"Detected Anomaly\")\n",
    "    ax[3].set_title(f\"Detection Result from {model_name}\")\n",
    "\n",
    "    fig.suptitle(f\"{model_name} on {column}\", fontweight=\"bold\", fontsize=14)\n",
    "    fig.savefig(f\"./results/{model_name}_{column}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for column in ts_column_to_be_used:\n",
    "    print(\n",
    "        f\"Accuracy of {model_name} on {column}: {accuracy_score(ground_truth, knncad_results[column])}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Absolute Deviation\n",
    "\n",
    "`pysad.models.StandardAbsoluteDeviation`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "sad_ts_values = defaultdict(list)\n",
    "sad_scores = defaultdict(list)\n",
    "sad_results = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pysad.models import StandardAbsoluteDeviation\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "UPPER_THRESHOLD = 1.6\n",
    "\n",
    "sad_model_map = defaultdict(lambda: StandardAbsoluteDeviation())\n",
    "\n",
    "\n",
    "def learning_and_detecting_anomaly(ts_column):\n",
    "    values = []\n",
    "    scores = []\n",
    "    results = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for _, row in anoml_iot_dataset.iterrows():\n",
    "        ts_value = row[ts_column]\n",
    "        features = np.array([row[ts_column]])\n",
    "        score = sad_model_map[ts_column].fit_score_partial(features)\n",
    "        scores.append(score)  # Save the score\n",
    "        result = get_decision_by_threshold(score, upper=UPPER_THRESHOLD)\n",
    "        values.append(ts_value)\n",
    "        results.append(result)  # Save the evaluation result\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\n",
    "        f\"\"\"[Performance of {ts_column}]\n",
    "Time taken for all record (s): {{:.5f}}\n",
    "Avg time taken for each (s)  : {{:.10f}}\n",
    "\"\"\".format(\n",
    "            end_time - start_time, (end_time - start_time) / record_count\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return values, scores, results\n",
    "\n",
    "\n",
    "pool = multiprocessing.Pool(processes=3)\n",
    "output = pool.map(learning_and_detecting_anomaly, ts_column_to_be_used)\n",
    "\n",
    "for idx, column in enumerate(ts_column_to_be_used):\n",
    "    sad_ts_values[column] = output[idx][0]\n",
    "    sad_scores[column] = output[idx][1]\n",
    "    sad_results[column] = output[idx][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    len(ts_column_to_be_used), 1, figsize=(20, 10), sharex=True, gridspec_kw={}\n",
    ")\n",
    "\n",
    "for idx, column in enumerate(ts_column_to_be_used):\n",
    "    ax[idx].plot(timestamps, sad_scores[column], label=column)\n",
    "    ax[idx].axhline(\n",
    "        y=UPPER_THRESHOLD, color=\"r\", linestyle=\"-\", label=\"Upper Threshold\"\n",
    "    )\n",
    "    ax[idx].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Chart for Comparing Ground Truth and Model Evaluation Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_name = \"Standard Absolute Deviation\"\n",
    "for column in ts_column_to_be_used:\n",
    "    fig, ax = plt.subplots(\n",
    "        4,\n",
    "        1,\n",
    "        figsize=(20, 10),\n",
    "        sharex=True,\n",
    "        gridspec_kw={\"height_ratios\": [7, 2, 1, 1]},\n",
    "    )\n",
    "    ax[0].plot(timestamps, sad_ts_values[column], label=column)\n",
    "    ax[0].legend()\n",
    "    ax[0].set_title(f\"Telemetry value of {column}\")\n",
    "    ax[1].plot(timestamps, sad_scores[column], label=\"Score\")\n",
    "    ax[1].axhline(y=UPPER_THRESHOLD, color=\"r\", linestyle=\"-\", label=\"Upper Threshold\")\n",
    "    ax[1].legend()\n",
    "    ax[1].set_title(f\"Score from {model_name}\")\n",
    "    ax[2].plot(timestamps, ground_truth, label=\"Ground Truth\")\n",
    "    ax[2].set_title(\"Ground Truth of Anomaly\")\n",
    "    ax[3].plot(timestamps, sad_results[column], label=\"Detected Anomaly\")\n",
    "    ax[3].set_title(f\"Detection Result from {model_name}\")\n",
    "\n",
    "    fig.suptitle(f\"{model_name} on {column}\", fontweight=\"bold\", fontsize=14)\n",
    "    fig.savefig(f\"./results/{model_name}_{column}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for column in ts_column_to_be_used:\n",
    "    print(\n",
    "        f\"Accuracy of {model_name} on {column}: {accuracy_score(ground_truth, sad_results[column])}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
